{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Attention Model\n",
    "## Author: Gary Corcoran\n",
    "## Date: Jan. 4th, 2017\n",
    "\n",
    "Attention Label Classiftion of Dashcam videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read Input Data\n",
    "Input data is stored in a numpy matrix consisting of each RGB image resized to $(100 \\times 100)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display inline figures\n",
    "%matplotlib inline\n",
    "\n",
    "# data manipulation helpers\n",
    "def shuffle(X, y):\n",
    "    \"\"\"\n",
    "    Shuffle Input Matrices.\n",
    "    \n",
    "    @param X: input data matrix [num_instances,num_seqs,width,height,depth]\n",
    "                @pre numpy matrix\n",
    "    @param y: input labels matrix [num_instances]\n",
    "                @pre numpy matrix\n",
    "                \n",
    "    @return shuffled data\n",
    "    \"\"\"\n",
    "    idx = np.random.permutation(len(X))\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    return X, y\n",
    "\n",
    "def split_normalize_data(X, y):\n",
    "    \"\"\"\n",
    "    Split Dataset and Normalize Data.\n",
    "    \n",
    "    @param X: input data matrix [num_instances,num_seqs,width,height,depth]\n",
    "                @pre numpy matrix\n",
    "    @param y: input labels matrix [num_instances]\n",
    "                @pre numpy matrix\n",
    "                \n",
    "    @return training, validation and test datasets\n",
    "    \"\"\"\n",
    "    X_train, X_val, X_test = np.split(X, [int(0.8*len(X)), len(X)])\n",
    "    y_train, y_val, y_test = np.split(y, [int(0.8*len(y)), len(X)])\n",
    "    print('X_train:', X_train.shape, 'y_train:', y_train.shape)\n",
    "    print('X_val:', X_val.shape, 'y_val:', y_val.shape)\n",
    "    print('X_test:', X_test.shape, 'y_test:', y_test.shape)\n",
    "    # normalize input data (0 mean, unit variance)\n",
    "    X_train, X_val, X_test = normalize(X_train, X_val, X_test)\n",
    "    training_data = X_train, y_train\n",
    "    validation_data = X_val, y_val\n",
    "    test_data = X_test, y_test\n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "def normalize(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Normalize Input Data.\n",
    "    \n",
    "    After normalization the training input data should have a mean 0 and a\n",
    "    standard deviation of 1.\n",
    "    \n",
    "    @param X_train: input training data\n",
    "    @param X_val:   input validation data\n",
    "    @param X_test:  input test data\n",
    "    \n",
    "    @return normalized input data\n",
    "    \"\"\"\n",
    "    # normalize training data\n",
    "    m = np.mean(np.mean(X_train, axis=0), axis=0)\n",
    "    X_train = np.asarray(X_train - m, dtype=np.float32)\n",
    "    std = np.std(X_train)\n",
    "    X_train /= std\n",
    "    # normalize validation data\n",
    "    X_val = np.asarray((X_val - m) / std, dtype=np.float32)\n",
    "    # normalize test data\n",
    "    X_test = np.asarray((X_test - m) / std, dtype=np.float32)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "# read input data\n",
    "data_path = '../data/'\n",
    "# X is dimensions [num_instances, num_seqs, width, height, depth]\n",
    "X = np.load(data_path + 'X_videos_med.npy')\n",
    "# y is dimensions [num_instances] (0=low attention, 1=medium attention,\n",
    "# 2=high attention, 3=very high attention)\n",
    "y = np.load(data_path + 'y_videos_med.npy')\n",
    "# shuffle data\n",
    "X, y = shuffle(X, y)\n",
    "# split data\n",
    "training_data, validation_data, test_data = split_normalize_data(X, y)\n",
    "X_train, y_train = training_data\n",
    "X_val, y_val = validation_data\n",
    "# X_test, y_test = test_data\n",
    "# number of examples\n",
    "n_train = len(X_train)\n",
    "n_val = len(X_val)\n",
    "# n_test = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Displayed Sampled Videos\n",
    "Images from video frame are sampled 20 frames apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display 5 sampled videos\n",
    "plt.figure(figsize=(20,20))\n",
    "for i, idx in enumerate(np.random.randint(len(X), size=5)):\n",
    "    vid = X[idx]\n",
    "    label = y[idx] + 1\n",
    "    for j, frame in enumerate(range(0, 100, 20)):\n",
    "        plt.subplot(5, 5, i*5+j+1)\n",
    "        plt.imshow(cv2.cvtColor(vid[frame], cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Level ' + str(label) + ' Frame ' + str(frame))\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build the Model -  Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network.\n",
    "    \n",
    "    @param input_size: number of input neurons\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, fc1_hidden):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1_hidden = fc1_hidden\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*22*22, fc1_hidden)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through network.\n",
    "        \n",
    "        @param x: input data\n",
    "        \n",
    "        @return output predictions\n",
    "        \"\"\"\n",
    "        # reshape into nSamples x nChannels x Height x Width\n",
    "        x = x.view(-1, *self.input_size)\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 16*22*22)\n",
    "        out = self.fc1(out)\n",
    "        # reshape into nSequence x nSamples x nFeatures\n",
    "        out = out.view(-1, 100, self.fc1_hidden)\n",
    "        out = torch.transpose(out, 0, 1)\n",
    "        return out\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network.\n",
    "    \n",
    "    @param input_size: number of input neurons\n",
    "    @param hidden_size: number of hidden neurons\n",
    "    @param output_size: number of output neurons\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, data, hidden):\n",
    "        combined = torch.cat((data, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, GPU):\n",
    "        if GPU:\n",
    "            return Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "        return Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Recurrent Neural Network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = CNN(input_size=(3,100,100), fc1_hidden=256)\n",
    "        self.rnn = RNN(input_size=256, hidden_size=64, output_size=4)\n",
    "    \n",
    "    def forward(self, data, hidden):\n",
    "        data_feats = self.cnn.forward(data)\n",
    "        num_seqs = data_feats.size()[0]\n",
    "        for i in range(num_seqs):\n",
    "            # pass through RNN\n",
    "            output, hidden = self.rnn.forward(data_feats[i], hidden)\n",
    "            \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, GPU):\n",
    "        return self.rnn.init_hidden(batch_size, GPU)\n",
    "\n",
    "# cnn = CNN(input_size=(3,100,100), fc1_hidden=256)\n",
    "# print(cnn)\n",
    "# rnn = RNN(input_size=100*100*3, hidden_size=8, output_size=4)\n",
    "# print(rnn)\n",
    "crnn = CRNN()\n",
    "print(crnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, num_epochs, batch_size, learning_rate, criterion, optimizer, GPU):\n",
    "    if GPU:\n",
    "        net = net.cuda()\n",
    "    training_losses = []\n",
    "    training_accuracy = []\n",
    "    validation_losses = []\n",
    "    validation_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch', epoch)\n",
    "        training_loss = 0.0\n",
    "        training_correct = 0\n",
    "        validation_loss = 0.0\n",
    "        validation_correct = 0\n",
    "        total_correct_train = 0\n",
    "        total_correct_val = 0\n",
    "        # generator to loop through mini-batches\n",
    "        mini_batch = ((X_train[k:k+batch_size], y_train[k:k+batch_size])\n",
    "                     for k in range(0, n_train, batch_size))\n",
    "        # each mini-batch\n",
    "        for x_batch, y_batch in mini_batch:\n",
    "            # store in variables\n",
    "            if GPU:\n",
    "                inputs = Variable(torch.from_numpy(x_batch).cuda())\n",
    "                targets = Variable(torch.from_numpy(y_batch).cuda())\n",
    "            else:\n",
    "                inputs = Variable(torch.from_numpy(x_batch))\n",
    "                targets = Variable(torch.from_numpy(y_batch))\n",
    "            # zero-out gradient\n",
    "            net.zero_grad()\n",
    "            # initialize hidden layer\n",
    "            hidden = net.init_hidden(batch_size, GPU)\n",
    "            # pass through network\n",
    "            output, hidden = net.forward(inputs, hidden)\n",
    "            # compute loss and backprop\n",
    "            loss = criterion(output, targets)\n",
    "            training_loss += loss.data[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # compute accuracy\n",
    "            _, y_pred = torch.max(output.data, 1)\n",
    "            correct = (y_pred == targets.data).sum()\n",
    "            training_correct += correct\n",
    "            total_correct_train += y_pred.size()[0]\n",
    "\n",
    "        # validation\n",
    "        # generator to loop through mini-batches\n",
    "        mini_batch = ((X_val[k:k+batch_size], y_val[k:k+batch_size])\n",
    "                     for k in range(0, n_val, batch_size))\n",
    "        # each mini-batch\n",
    "        for x_batch, y_batch in mini_batch:\n",
    "            # store in variables\n",
    "            if GPU:\n",
    "                inputs = Variable(torch.from_numpy(x_batch).cuda())\n",
    "                targets = Variable(torch.from_numpy(y_batch).cuda())\n",
    "            else:\n",
    "                inputs = Variable(torch.from_numpy(x_batch))\n",
    "                targets = Variable(torch.from_numpy(y_batch))\n",
    "            # initialize hidden layer\n",
    "            hidden = net.init_hidden(batch_size, GPU)\n",
    "            # pass through netword\n",
    "            output, hidden = net.forward(inputs, hidden)\n",
    "            # compute loss\n",
    "            loss = criterion(output, targets)\n",
    "            validation_loss += loss.data[0]\n",
    "            # compute accuracy\n",
    "            _, y_pred = torch.max(output.data, 1)\n",
    "            correct = (y_pred == targets.data).sum()\n",
    "            validation_correct += correct\n",
    "            total_correct_val += y_pred.size()[0]\n",
    "\n",
    "        # print statistics\n",
    "        print('\\tTraining Loss:', training_loss)\n",
    "        training_losses.append(training_loss)\n",
    "        training_acc = training_correct / total_correct_train * 100\n",
    "        print('\\tTraining Accuracy:', training_acc)\n",
    "        training_accuracy.append(training_acc)\n",
    "        print('\\tValidation Loss:', validation_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_acc = validation_correct / total_correct_val * 100\n",
    "        print('\\tValidation Accuracy:', validation_acc)\n",
    "        validation_accuracy.append(validation_acc)\n",
    "    \n",
    "    return training_losses, validation_losses, training_accuracy, validation_accuracy\n",
    "\n",
    "net = crnn\n",
    "GPU = True\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.0001\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "# start timer\n",
    "start = time.time()\n",
    "# training function\n",
    "training_losses, validation_losses, training_accuracy, validation_accuracy = train(net, \n",
    "        num_epochs, batch_size, learning_rate, criterion, optimizer, GPU)\n",
    "print('Elapsed Time:', time.time() - start)\n",
    "\n",
    "# plot figures\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(training_losses, label='train'), plt.plot(validation_losses, label='validation')\n",
    "plt.title('Losses'), plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.subplot(122)\n",
    "plt.plot(training_accuracy, label='train'), plt.plot(validation_accuracy, label='valid')\n",
    "plt.ylim(0, 100)\n",
    "plt.title('Accuracy'), plt.xlabel('Epoch'), plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Flow\n",
    "### Read Flow Input Data\n",
    "Flow input data is stored in a numpy matrix of dimensions $(100 \\times 100 \\times 2)$ (i.e. Width, Height, and flow in X and Y directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display inline figures\n",
    "%matplotlib inline\n",
    "\n",
    "# data manipulation helpers\n",
    "def shuffle(X, y):\n",
    "    \"\"\"\n",
    "    Shuffle Input Matrices.\n",
    "    \n",
    "    @param X: input data matrix [num_instances,num_seqs,width,height,depth]\n",
    "                @pre numpy matrix\n",
    "    @param y: input labels matrix [num_instances]\n",
    "                @pre numpy matrix\n",
    "                \n",
    "    @return shuffled data\n",
    "    \"\"\"\n",
    "    idx = np.random.permutation(len(X))\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    return X, y\n",
    "\n",
    "def split_normalize_data(X, y):\n",
    "    \"\"\"\n",
    "    Split Dataset and Normalize Data.\n",
    "    \n",
    "    @param X: input data matrix [num_instances,num_seqs,width,height,depth]\n",
    "                @pre numpy matrix\n",
    "    @param y: input labels matrix [num_instances]\n",
    "                @pre numpy matrix\n",
    "                \n",
    "    @return training, validation and test datasets\n",
    "    \"\"\"\n",
    "    X_train, X_val, X_test = np.split(X, [int(0.8*len(X)), len(X)])\n",
    "    y_train, y_val, y_test = np.split(y, [int(0.8*len(y)), len(X)])\n",
    "    print('X_train:', X_train.shape, 'y_train:', y_train.shape)\n",
    "    print('X_val:', X_val.shape, 'y_val:', y_val.shape)\n",
    "    print('X_test:', X_test.shape, 'y_test:', y_test.shape)\n",
    "    # normalize input data (0 mean, unit variance)\n",
    "    X_train, X_val, X_test = normalize(X_train, X_val, X_test)\n",
    "    training_data = X_train, y_train\n",
    "    validation_data = X_val, y_val\n",
    "    test_data = X_test, y_test\n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "def normalize(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Normalize Input Data.\n",
    "    \n",
    "    After normalization the training input data should have a mean 0 and a\n",
    "    standard deviation of 1.\n",
    "    \n",
    "    @param X_train: input training data\n",
    "    @param X_val:   input validation data\n",
    "    @param X_test:  input test data\n",
    "    \n",
    "    @return normalized input data\n",
    "    \"\"\"\n",
    "    # normalize training data\n",
    "    m = np.mean(np.mean(X_train, axis=0), axis=0)\n",
    "    X_train = np.asarray(X_train - m, dtype=np.float32)\n",
    "    std = np.std(X_train)\n",
    "    X_train /= std\n",
    "    # normalize validation data\n",
    "    X_val = np.asarray((X_val - m) / std, dtype=np.float32)\n",
    "    # normalize test data\n",
    "    X_test = np.asarray((X_test - m) / std, dtype=np.float32)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "# read input data\n",
    "data_path = '../data/'\n",
    "# X is dimensions [num_instances, num_seqs, width, height, depth]\n",
    "X = np.load(data_path + 'X_flow_big.npy')\n",
    "# y is dimensions [num_instances] (0=low attention, 1=medium attention,\n",
    "# 2=high attention, 3=very high attention)\n",
    "y = np.load(data_path + 'y_flow_big.npy')\n",
    "# shuffle data\n",
    "X, y = shuffle(X, y)\n",
    "# split data\n",
    "training_data, validation_data, test_data = split_normalize_data(X, y)\n",
    "X_train, y_train = training_data\n",
    "X_val, y_val = validation_data\n",
    "# X_test, y_test = test_data\n",
    "# number of examples\n",
    "n_train = len(X_train)\n",
    "n_val = len(X_val)\n",
    "# n_test = len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sampled Optical Flow\n",
    "Images from video frame are sampled 20 frames apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 5 sampled videos\n",
    "hsv = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "hsv[:, :, 1] = 255\n",
    "plt.figure(figsize=(20,20))\n",
    "for i, idx in enumerate(np.random.randint(len(X), size=5)):\n",
    "    vid = X[idx]\n",
    "    label = y[idx] + 1\n",
    "    for j, frame in enumerate(range(0, 100, 20)):\n",
    "        plt.subplot(5, 5, i*5+j+1)\n",
    "        mag, ang = cv2.cartToPolar(vid[frame][:, :, 0], vid[frame][:, :, 1])\n",
    "        hsv[:, :, 0] = ang*180/np.pi/2\n",
    "        hsv[:, :, 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "        plt.imshow(rgb)\n",
    "        plt.title('Level ' + str(label) + ' Frame ' + str(frame))\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model -  Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network.\n",
    "    \n",
    "    @param input_size: number of input neurons\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, fc1_hidden):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1_hidden = fc1_hidden\n",
    "        self.conv1 = nn.Conv2d(2, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*22*22, fc1_hidden)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through network.\n",
    "        \n",
    "        @param x: input data\n",
    "        \n",
    "        @return output predictions\n",
    "        \"\"\"\n",
    "        # reshape into nSamples x nChannels x Height x Width\n",
    "        x = x.view(-1, *self.input_size)\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 16*22*22)\n",
    "        out = self.fc1(out)\n",
    "        # reshape into nSequence x nSamples x nFeatures\n",
    "        out = out.view(-1, 99, self.fc1_hidden)\n",
    "        out = torch.transpose(out, 0, 1)\n",
    "        return out\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network.\n",
    "    \n",
    "    @param input_size: number of input neurons\n",
    "    @param hidden_size: number of hidden neurons\n",
    "    @param output_size: number of output neurons\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, data, hidden):\n",
    "        combined = torch.cat((data, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, GPU):\n",
    "        if GPU:\n",
    "            return Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "        return Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Recurrent Neural Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_hidden, rnn_hidden):\n",
    "        super().__init__()\n",
    "        self.cnn = CNN(input_size=(2,100,100), fc1_hidden=cnn_hidden)\n",
    "        self.rnn = RNN(input_size=cnn_hidden, hidden_size=rnn_hidden, output_size=4)\n",
    "    \n",
    "    def forward(self, data, hidden):\n",
    "        data_feats = self.cnn.forward(data)\n",
    "        num_seqs = data_feats.size()[0]\n",
    "        for i in range(num_seqs):\n",
    "            # pass through RNN\n",
    "            output, hidden = self.rnn.forward(data_feats[i], hidden)\n",
    "            \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, GPU):\n",
    "        return self.rnn.init_hidden(batch_size, GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, num_epochs, batch_size, learning_rate, criterion, optimizer, GPU):\n",
    "    if GPU:\n",
    "        net = net.cuda()\n",
    "    training_losses = []\n",
    "    training_accuracy = []\n",
    "    validation_losses = []\n",
    "    validation_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch', epoch)\n",
    "        training_loss = 0.0\n",
    "        training_correct = 0\n",
    "        validation_loss = 0.0\n",
    "        validation_correct = 0\n",
    "        total_correct_train = 0\n",
    "        total_correct_val = 0\n",
    "        # generator to loop through mini-batches\n",
    "        mini_batch = ((X_train[k:k+batch_size], y_train[k:k+batch_size])\n",
    "                     for k in range(0, n_train, batch_size))\n",
    "        # each mini-batch\n",
    "        for x_batch, y_batch in mini_batch:\n",
    "            # store in variables\n",
    "            if GPU:\n",
    "                inputs = Variable(torch.from_numpy(x_batch).cuda())\n",
    "                targets = Variable(torch.from_numpy(y_batch).cuda())\n",
    "            else:\n",
    "                inputs = Variable(torch.from_numpy(x_batch))\n",
    "                targets = Variable(torch.from_numpy(y_batch))\n",
    "            # zero-out gradient\n",
    "            net.zero_grad()\n",
    "            # initialize hidden layer\n",
    "            hidden = net.init_hidden(batch_size, GPU)\n",
    "            # pass through network\n",
    "            output, hidden = net.forward(inputs, hidden)\n",
    "            # compute loss and backprop\n",
    "            loss = criterion(output, targets)\n",
    "            training_loss += loss.data[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # compute accuracy\n",
    "            _, y_pred = torch.max(output.data, 1)\n",
    "            correct = (y_pred == targets.data).sum()\n",
    "            training_correct += correct\n",
    "            total_correct_train += y_pred.size()[0]\n",
    "\n",
    "        # validation\n",
    "        # generator to loop through mini-batches\n",
    "        mini_batch = ((X_val[k:k+batch_size], y_val[k:k+batch_size])\n",
    "                     for k in range(0, n_val, batch_size))\n",
    "        # each mini-batch\n",
    "        for x_batch, y_batch in mini_batch:\n",
    "            # store in variables\n",
    "            if GPU:\n",
    "                inputs = Variable(torch.from_numpy(x_batch).cuda())\n",
    "                targets = Variable(torch.from_numpy(y_batch).cuda())\n",
    "            else:\n",
    "                inputs = Variable(torch.from_numpy(x_batch))\n",
    "                targets = Variable(torch.from_numpy(y_batch))\n",
    "            # initialize hidden layer\n",
    "            hidden = net.init_hidden(batch_size, GPU)\n",
    "            # pass through netword\n",
    "            output, hidden = net.forward(inputs, hidden)\n",
    "            # compute loss\n",
    "            loss = criterion(output, targets)\n",
    "            validation_loss += loss.data[0]\n",
    "            # compute accuracy\n",
    "            _, y_pred = torch.max(output.data, 1)\n",
    "            correct = (y_pred == targets.data).sum()\n",
    "            validation_correct += correct\n",
    "            total_correct_val += y_pred.size()[0]\n",
    "\n",
    "        # print statistics\n",
    "        print('\\tTraining Loss:', training_loss)\n",
    "        training_losses.append(training_loss)\n",
    "        training_acc = training_correct / total_correct_train * 100\n",
    "        print('\\tTraining Accuracy:', training_acc)\n",
    "        training_accuracy.append(training_acc)\n",
    "        print('\\tValidation Loss:', validation_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_acc = validation_correct / total_correct_val * 100\n",
    "        print('\\tValidation Accuracy:', validation_acc)\n",
    "        validation_accuracy.append(validation_acc)  \n",
    "    \n",
    "    return training_losses, validation_losses, training_accuracy, validation_accuracy\n",
    "\n",
    "sizes = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "for s in sizes:\n",
    "    crnn = CRNN(cnn_hidden=s, rnn_hidden=s//2)\n",
    "    print(crnn)\n",
    "    \n",
    "    net = crnn\n",
    "    GPU = True\n",
    "    num_epochs = 30\n",
    "    batch_size = 10\n",
    "    learning_rate = 0.0001\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "    # training function\n",
    "    training_losses, validation_losses, training_accuracy, validation_accuracy = train(net, \n",
    "            num_epochs, batch_size, learning_rate, criterion, optimizer, GPU)\n",
    "    print('Elapsed Time:', time.time() - start)\n",
    "\n",
    "    # plot figures\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(training_losses, label='train'), plt.plot(validation_losses, label='validation')\n",
    "    plt.title('Losses'), plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(training_accuracy, label='train'), plt.plot(validation_accuracy, label='valid')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.title('Accuracy'), plt.xlabel('Epoch'), plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
